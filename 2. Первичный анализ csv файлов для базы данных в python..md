
## 2. Первичный анализ данных в среде разработки Jupyter Notebook с использованием библиотеки Pandas.

> Перед загрузкой CSV файлов в MySQL, полезно провести первичный анализ данных. Это позволяет изучить качество данных, выявить потенциальные проблемы и подготовить данные для успешной загрузки и использования. Также, указывая наиболее точно размеры полей в таблицах БД, можно оптимизировать размер памяти, который займет БД после загрузки.

Таким образом, первичный анализ данных будет включать:

1. Подсчет максимального количества символов в одной ячейке данных, которое может быть в заданном столбце;
   1.1 Выведение ячейки с максимальной длиной по заданной длине. Для проверки работы кода.
2. Ознакомление с примерным содержимым столбца через вывод значений нескольких ячеек, выбранных случайным образом;
3. Подсчет количества всех имеющихся уникальных значений столбца таблицы;
4. Вывод списка всех заголовков столбцов на консоль;
5. Вывод заданного количества строк таблицы для дальнейшего ознакомления;
6. Вывод количества пустых (не имеющих значений) ячеек в указанном столбце. 
7. Подсчет общего количества строк таблицы;
8. Изучение типов данных в каждом из столбцов.

Следует учитывать, что данная база создается исключительно для анализа и визуализации имеющихся данных и не предполагает дальнейшей с ней работы как с действующей базой данных.

При работе с достаточно большими файлами, а в нашем случае речь идет, например, о файлах размером 20.1 GiB (vacansies) или 8.2 GiB (curricula_vitae), могут быть сложности из-за ограничений в объеме оперативной памяти рабочего компьютера или ноутбука.
Для оптимизации работы с данными файлами можно использовать генераторы и делить файлы на части (chunks) для последующей работы и объединения данных, при необходимости.

> Генератор – это удобный механизм конструирования итерируемого объекта, похожий на обычную функцию. Если обычная функция воз-
вращает единственное значение, то генератор может возвращать последовательность значений, приостанавливаться и работать дальше в ожидании запроса следующего. При создании генератора вместо return используется yield. Поскольку генераторы отдают значения по одному, а не весь список сразу, программа потребляет меньше памяти. Например, при каждой итерации цикла for значение возвращается с помощью оператора yield, а затем функция продолжает выполнение со следующей итерации, пока не иссякнет.

Для последовательного чтения файлов небольшими порциями можно:
- задать размер порций для последовательной обработки каждой (chunks); 
- задать размер одной порции (chunksize());
- ограничить вывод количества строк таблицы для ознакомления или обработки (nrows);
- использовать метод get_chunk для чтения кусков произвольного размера;

Для контроля за временем в течении которого в ячейке среды разработки Jupyter Notebook выполняется нужный фрагмент кода, можно использовать ipython-autotime. Для установки необходимо запустить в ячейке следующий фрагмент ```pip install ipython-autotime```.
В первый раз для запуска подойдет следующий код: ```%load_ext autotime``` или ```%reload_ext autotime``` при повторном запуске. Если в начале ячейки стоит ```%load_ext autotime``` или ```%reload_ext autotime```, при запуске ячейки начинается отсчет времени выполнения соответствующего кода. В случае, когда счетчик необходимо отключить, код будет выглядеть вот так: ```%unload_ext autotime```.

### 2.1 Подсчет максимального количества символов в одной ячейке данных, которое может быть в заданном столбце.

В процессе создания базы данных возникает вопрос оптимизации размера памяти, которое эта база займет. Ограничение размера поля для хранения данных в таблице при назначении типа данных столбца и является одним из вариантов решения этого вопроса. Для того, чтобы знать, насколько можно ограничить размер поля в таблице MySQL, можно использовать следующий код:
Параметр ```low_memory=False``` заставляет pandas анализировать все строки файла CSV для определения типов данных, что может занять больше времени и памяти, но гарантирует более точное определение типов данных и более эффективное использование памяти.
По умолчанию, когда ```low_memory=True```, pandas автоматически определяет тип данных для каждого столбца на основе первых нескольких строк файла CSV. Однако, если файл очень большой или содержит столбцы с разными типами данных(а в нашем случае это именно так), это может привести к неправильной классификации типов данных и использованию большего количества памяти.

```python
%load_ext autotime
import pandas as pd

file = 'file_path'

def read_csv(file_name):
    for chunk in pd.read_csv(file_name, sep=';', chunksize=100_000, low_memory=False):
        yield chunk 

def process_dataframe(dataframe):
    return dataframe['column_name'].tolist()

m = 0
for df in read_csv(file):
    lst = process_dataframe(df)
    x = max([len(str(lst[i]))for i in range(len(lst))])
    if x > m:
        print(x)
        m = x
print('---' *  20)
print(m)

```
При обработке каждого из чанков данный фрагмент кода находит максимальное значение длины строки ячейки и, при условии, что данное значение больше предыдущего, выводит его на консоль. 
Метод .tolist() в библиотеке pandas используется для преобразования данных из объекта DataFrame или Series в список.
Вместо "column_name" может быть наименование любой из колонок датафрейма, максимальную длину ячеек которой необходимо найти.
Завершением работы кода является вывод максимального значения длины строки и времени, в течении которого выполнялся данный фрагмент.

#### 2.1.1 Выведение ячейки с максимальной длиной по заданной длине. Для проверки работы кода.

Иногда в базе встречаются неожиданные данные. Для проверки правильно ли код находит размер самой длиной строки и для ее просмотра, можно указать найденную ранее длину строки и вывести саму строку на консоль.
Данный код сначала создает переменную, в которую сохраняет чанк, в котором находится ячейка с указанной длиной, после чего выходит из цикла и распечатывает содержимое найденной ячейки как элемент списка.
Длина строки в данном коде задается в двух местах - вместо n.

```python
import pandas as pd
%reload_ext autotime

import pandas as pd

file = 'file_path'

def read_csv(file_name):
    for chunk in pd.read_csv(file_name, sep=';', chunksize=10_000, low_memory=False):
        yield chunk 

def process_dataframe(dataframe):
    return dataframe['column_name'].tolist()

for df in read_csv(file):
    lst = process_dataframe(df)
    x = max([len(str(lst[i])) for i in range(len(lst))])
    if x == n:
        data = df['column_name']
        break
        
print([j for j in data if len(str(j)) == n])
```

### 2.2 Ознакомление с примерным содержимым столбца через вывод значений нескольких ячеек, выбранных случайным образом.

Следующий фрагмент задает размер чанка для обработки и выводит из него указанное количество случайных строк, для того, чтобы ознакомиться с примерным содержанием конкретного столбца таблицы. Минус в том, что чанк берет первые n указанных строк таблицы. Может оказаться, что чанк будет содержать данные за 2019 год, при том что данные за последующие годы будут сильно отличаться.
В данном случае экономия памяти происходит еще и за счет того, что метод pandas read_csv() считывает данные только из указанного столбца, игнорируя остальные. Для этого мы указываем следующий аргумент: usecols=['column_name']. 
Метод .sample() в библиотеке pandas используется для случайной выборки определенного количества строк датафрейма. В данном случае введен параметр 5, который обозначает количество данных из ячеек, которое нужно вывести на консоль.
Параметр iterator=True, возвращает объект-итератор, который можно использовать для последовательного доступа к данным по частям. Это полезно, когда файл очень большой и не помещается целиком в память. Использование итератора позволяет обрабатывать данные по мере их поступления, что экономит память и улучшает производительность.

```python
%reload_ext autotime
reader = pd.read_csv('file_path', sep=';', iterator=True, usecols=['column_name'])
reader.get_chunk(1_000_000)['column_name'].sample(5)
```
### 2.3 Подсчет количества всех имеющихся уникальных значений столбца таблицы.

Иногда столбец таблицы содержит только несколько значений повторяющихся данных, что соответствует типу данных ENUM() в БД MySQL.
Для того, чтобы вывести на консоль все эти значения, их можно посчитать.
Метод .value_counts() в библиотеке pandas используется для подсчета уникальных значений в столбце или серии данных. Он возвращает объект типа Series, где уникальные значения становятся индексами, а количество вхождений каждого значения становится значениями.

```python
%reload_ext autotime
reader = pd.read_csv('file_path', sep=';', iterator=True, usecols=['column_name'])
reader.get_chunk(1_000_000)['column_name'].value_counts()
```

### 2.4 Вывод списка всех заголовков столбцов на консоль.

Данный фрагмент выводит все названия колонок одним списком. Список может понадобиться, например, для создания схемы базы данных. Количество строк ограничено параметром nrows для того, чтобы избежать перезагрузки оперативной памяти, когда файл большой. 

```python
import pandas as pd
file = 'file_path'
pd.read_csv(file, sep=';', nrows=1).columns
```

### 2.5 Вывод заданного количества строк таблицы для дальнейшего ознакомления.

Уже знакомы код, в котором убрали атрибут .columns, который в библиотеке pandas используется для получения списка названий столбцов датафрейма. Параметр nrows=5 выведет на консоль 5 строк таблицы целиком, а также заголовки столбцов.

```python
import pandas as pd

file = 'file_path'
pd.read_csv(file, sep=';', nrows=5)
```
### 2.6 Вывод количества пустых (не имеющих значений) ячеек в указанном столбце. 

Через объект TextParser, класс в библиотеке pandas, используемый для чтения текстовых файлов и преобразования их в объекты DataFrame, данный код отработает достаточно быстро, размер чанка здесь не нужен.

```python
%reload_ext autotime
reader = pd.read_csv('file_path', sep=';', iterator=True, usecols=['column_name'])
reader.get_chunk()['column_name'].isnull().sum()
```
### 2.7 Подсчет общего количества строк таблицы.

Данный код суммирует итоговое количество строк по каждому чанку и выводит итоговый результат на консоль. 

```python
%reload_ext autotime
import pandas as pd

file = 'file_path'

import pandas as pd

def read_csv(file_name):
    for chunk in pd.read_csv(file_name, sep=';', chunksize=50_000, low_memory=False):
        yield chunk

def process_dataframe(dataframe):
    return dataframe.shape[0]

result = 0
for df in read_csv(file):
    result += process_dataframe(df)
print(result)
```
### 2.8 Изучение типов данных в каждом из столбцов.

Типы данных, которые выявляет pandas в данной базе очень специфические: есть не соответствие между данными непосредственно находящимися в заданном столбце и тем, какой тип данных выявляет pandas. В некоторых столбцах могут находиться сразу два разных типа данных. Поэтому в данном случае более точно определиться можно будет уже в MySQL, а данные загрузить в VARCHAR или TEXT, по возможности ограничив количество знаков, которые могут быть в ячейке для оптимизации памяти ноутбука.
Следующий код подходит для определения типа данных столбца датафрейма.
```
import pandas as pd
df = pd.read_csv('file_path', sep=';', usecols=['column_name'])
df.dtypes
```
