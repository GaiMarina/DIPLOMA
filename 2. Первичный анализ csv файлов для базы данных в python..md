
## 2. Первичный анализ csv файлов для базы данных в jupyter notebook.

---
> Для создания схемы базы данных необходимо провести первичный анализ, а именно:

- узнать тип данных в каждом из столбцов таблиц;
- вывести фактическую максимальную длину строки в ячейке для ограничения объема памяти,
которую займет база данных;
- вывести ячейку с максимальной длинной по имеющемуся индексу для проверки работы кода.
- получить информацию из нескольких ячеек, выбранных случайным образом, 
для ознакомления с примерным содержимым таблицы;
- посчитать количество всех имеющихся значений столбца в случае необходимости;
- вывести все названия столбцов для копирования;
- вывести заданное количество строк таблицы для дальнейшего ознакомления
- узнать общее количество строк датафрейма из csv файла.

> Следует учитывать, что данная база создается исключительно для анализа и 
визуализации имеющихся данных и не предполагает дальнейшей с ней работы как с 
> действующей базой данных.

> При работе с достаточно большими файлами, а в нашем случае речь идет, например, о
файлах размером 20.1 GiB (vacansies) или 8.2 GiB (curricula_vitae), могут быть 
сложности из-за ограничений в объеме оперативной памяти рабочего компьютера или ноутбука.
> Для оптимизации работы с данными файлами можно использовать генераторы и делить файлы
на части (chunks) для последующей работы с файлом по частям и дальнейшего объединения 
данных если это необходимо.
> Генератор – это удобный механизм конструирования итерируемого объек-
та, похожий на обычную функцию. Если обычная функция воз-
вращает единственное значение, то генератор может возвращать последова-
тельность значений, приостанавливаться и работать дальше в ожидании 
запроса следующего. При создании генератора вместо return используется yield.
Поскольку генераторы отдают значения по одному, а не весь список сразу, 
программа потребляет меньше памяти.
> Для последовательного чтения файлов небольшими порциями можно:
- задать размер порций для последовательной обработки каждой (chunks); 
- задать размер одной порции для дальнейшей обработки (chunksize());
- ограничить вывод количества строк таблицы для ознакомления или обработки (nrows);
- использовать метод get_chunk для чтения кусков произвольного размера.
> Для контроля за временем в течении которого в ячейке среды разработки Jupyter Notebook
выполняется нужный фрагмент кода, можно использовать ipython-autotime 
(%load_ext autotime или %reload_ext autotime). 

> Данный код используется для нахождения максимальной длины строки в ячейке. 
> При обработке каждого из чанков данный фрагмент кода находит максимальное значение 
длины строки ячейки и, при условии, что данное значение больше предыдущего, выводит
его на консоль. 
> Завершением работы кода является вывод максимального значения длины строки и времени, 
в течении которого выполнялся данный фрагмент.

```python
%load_ext autotime
import pandas as pd

file = '/media/marina/big/Диплом/diploma_data/vacancies.csv'

def read_csv(file_name):
    for chunk in pd.read_csv(file_name, sep=';', chunksize=100_000, low_memory=False):
        yield chunk 

def process_dataframe(dataframe):
    return dataframe['work_places'].tolist()

m = 0
for df in read_csv(file):
    lst = process_dataframe(df)
    x = max([len(str(lst[i]))for i in range(len(lst))])
    if x > m:
        print(x)
        m = x
print('---' *  20)
print(m)

```
> Следующий фрагмент задает размер чанка для обработки и выводит из него указанное 
количество случайных строк, для того, чтобы ознакомиться с примерным содержанием
конкретного столбца таблицы.

```python
%reload_ext autotime
reader = pd.read_csv('/media/marina/big/Диплом/diploma_data/vacancies.csv', sep=';', iterator=True, usecols=['work_places'])
reader.get_chunk(1_000_000)['work_places'].sample(5)
```
> В данном случае производится подсчет количества всех возможных значений столбца из 
csv таблицы.

```python
%reload_ext autotime
reader = pd.read_csv('/media/marina/big/Диплом/diploma_data/vacancies.csv', sep=';', iterator=True, usecols=['time_change_inner_info'])
reader.get_chunk(1_000_000)['time_change_inner_info'].value_counts()
```
> Иногда в данной базе встречаются неожиданные данные. Для проверки правильно
ли код находит размер самой длинной строки и для ее просмотра, можно вывести эту самую
строку, указав найденную ранее длину.
> Данный код сначала создает переменную, в которую сохраняет чанк, в котором находится
ячейка с указанной длиной, после чего выходит из цикла и распечатывает содержимое найденной
ячейки как элемент списка.

```python
import pandas as pd
%reload_ext autotime

import pandas as pd

file = '/media/marina/big/Диплом/diploma_data/vacancies.csv'

def read_csv(file_name):
    for chunk in pd.read_csv(file_name, sep=';', chunksize=10_000, low_memory=False):
        yield chunk 

def process_dataframe(dataframe):
    return dataframe['metro_station'].tolist()

for df in read_csv(file):
    lst = process_dataframe(df)
    x = max([len(str(lst[i])) for i in range(len(lst))])
    if x == 2286:
        data = df['metro_station']
        break
        
print([j for j in data if len(str(j)) == 2286])
```

> Данный фрагмент выводит все названия колонок одним списком. Список нужен, например, 
для создания схемы базы данных. Количество строк ограничено (nrows=5) для того, чтобы 
избежать перезагрузки оперативной памяти, когда файл большой. Если .columns убрать,
фрагмент распечатает указанное количество строк из csv файла.

```python
import pandas as pd

file = '/media/marina/big/Диплом/diploma_data/invitations.csv'
pd.read_csv(file, sep=';', nrows=5).columns
```
> Вывод количества пустых (не имеющих значений) ячеек в указанном столбце. 
Размер чанка не задан, но, через объект TextParser работает и так.

```python
%reload_ext autotime
reader = pd.read_csv('/media/marina/big/Диплом/diploma_data/vacancies.csv', sep=';', iterator=True, usecols=['caring_workers'])
reader.get_chunk()['caring_workers'].isnull().sum()
```
> Находим общее количество строк в csv файле. После обработки чанка используется 
двухсекундное засыпание (sleep(2)).

```python
%reload_ext autotime
from time import sleep 
import pandas as pd

file = '/media/marina/big/Диплом/diploma_data/vacansies.csv'

import pandas as pd

def read_csv(file_name):
    for chunk in pd.read_csv(file_name, sep=';', chunksize=50_000, low_memory=False):
        yield chunk

def process_dataframe(dataframe):
    return dataframe.shape[0]

result = 0
for df in read_csv(file):
    result += process_dataframe(df)
    sleep(2)
print(result)
```
